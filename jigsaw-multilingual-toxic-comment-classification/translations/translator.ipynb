{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /usr/local/anaconda3/lib/python3.7/site-packages (2.9.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (2019.11.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.7.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/anaconda3/lib/python3.7/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already up-to-date: mosestokenizer in /usr/local/anaconda3/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already up-to-date: packaging in /usr/local/anaconda3/lib/python3.7/site-packages (20.4)\n",
      "Requirement already satisfied, skipping upgrade: openfile in /usr/local/anaconda3/lib/python3.7/site-packages (from mosestokenizer) (0.0.7)\n",
      "Requirement already satisfied, skipping upgrade: docopt in /usr/local/anaconda3/lib/python3.7/site-packages (from mosestokenizer) (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: toolwrapper in /usr/local/anaconda3/lib/python3.7/site-packages (from mosestokenizer) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: uctools in /usr/local/anaconda3/lib/python3.7/site-packages (from mosestokenizer) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/anaconda3/lib/python3.7/site-packages (from packaging) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from packaging) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install transformers --upgrade\n",
    "!pip install mosestokenizer packaging --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "src_text = [\n",
    "    '>>fr<< This is a sentence in english that we want to translate to french.',\n",
    "    '>>pt<< This should go to portuguese.',\n",
    "    '>>es<< And this to Spanish.'\n",
    "]\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "# see tokenizer.supported_language_codes for choices\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour, jeune garÃ§on.']\n"
     ]
    }
   ],
   "source": [
    "#@title Translate with Transformers\n",
    "english_text = \"Good morning, young lad\" #@param {type:\"string\"}\n",
    "tgt_language = \"fr\" #@param [\"fr\", \"es\", \"it\", \"pt\", \"pt_br\", \"ro\", \"ca\", \"gl\", \"pt_BR\", \"la\", \"wa\", \"fur\", \"oc\", \"fr_CA\", \"sc\", \"es_ES\", \"es_MX\", \"es_AR\", \"es_PR\", \"es_UY\", \"es_CL\", \"es_CO\", \"es_CR\", \"es_GT\", \"es_HN\", \"es_NI\", \"es_PA\", \"es_PE\", \"es_VE\", \"es_DO\", \"es_EC\", \"es_SV\", \"an\", \"pt_PT\", \"frp\", \"lad\", \"vec\", \"fr_FR\", \"co\", \"it_IT\", \"lld\", \"lij\", \"lmo\", \"nap\", \"rm\", \"scn\", \"mwl\"] {allow-input: true}\n",
    "\n",
    "src_txt = f'>>{tgt_language}<< {english_text}'\n",
    "translated = model.generate(**tokenizer.prepare_translation_batch([src_txt]))\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                       comment_text  toxic lang\n",
      "0  59848  This is so cool. It's like, 'would you want yo...      0   en\n",
      "1  59849  Thank you!! This would make my life a lot less...      0   en\n",
      "2  59852  This is such an urgent design problem; kudos t...      0   en\n",
      "3  59855  Is this something I'll be able to install on m...      0   en\n",
      "4  59856               haha you guys are a bunch of losers.      1   en\n"
     ]
    }
   ],
   "source": [
    "# fn = \"jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\"\n",
    "fn = \"jigsaw-multilingual-toxic-comment-classification/train_data.csv\"\n",
    "df = pd.read_csv(fn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3149081\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1817456\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                       comment_text  toxic lang\n",
      "0  59848  This is so cool. It's like, 'would you want yo...      0   en\n",
      "1  59849  Thank you!! This would make my life a lot less...      0   en\n",
      "2  59852  This is such an urgent design problem; kudos t...      0   en\n",
      "3  59855  Is this something I'll be able to install on m...      0   en\n",
      "4  59856               haha you guys are a bunch of losers.      1   en\n"
     ]
    }
   ],
   "source": [
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\n",
      "idx: 100\n",
      "idx: 200\n",
      "idx: 300\n",
      "idx: 400\n",
      "idx: 500\n",
      "idx: 600\n",
      "idx: 700\n",
      "idx: 800\n",
      "idx: 900\n",
      "idx: 1000\n",
      "idx: 1100\n",
      "idx: 1200\n",
      "idx: 1300\n",
      "idx: 1400\n",
      "idx: 1500\n",
      "idx: 1600\n",
      "idx: 1700\n",
      "idx: 1800\n",
      "idx: 1900\n",
      "idx: 2000\n",
      "idx: 2100\n",
      "idx: 2200\n",
      "idx: 2300\n",
      "idx: 2400\n",
      "idx: 2500\n",
      "idx: 2600\n",
      "idx: 2700\n",
      "idx: 2800\n",
      "idx: 2900\n",
      "idx: 3000\n",
      "idx: 3100\n",
      "idx: 3200\n",
      "idx: 3300\n",
      "idx: 3400\n",
      "idx: 3500\n",
      "idx: 3600\n",
      "idx: 3700\n",
      "idx: 3800\n",
      "idx: 3900\n",
      "idx: 4000\n",
      "idx: 4100\n",
      "idx: 4200\n",
      "idx: 4300\n",
      "idx: 4400\n",
      "idx: 4500\n",
      "idx: 4600\n",
      "idx: 4700\n",
      "idx: 4800\n",
      "idx: 4900\n",
      "idx: 5000\n",
      "idx: 5100\n",
      "idx: 5200\n",
      "idx: 5300\n",
      "idx: 5400\n",
      "idx: 5500\n",
      "idx: 5600\n",
      "idx: 5700\n",
      "idx: 5800\n",
      "idx: 5900\n",
      "idx: 6000\n",
      "idx: 6100\n",
      "idx: 6200\n",
      "idx: 6300\n",
      "idx: 6400\n",
      "idx: 6500\n",
      "idx: 6600\n",
      "idx: 6700\n",
      "idx: 6800\n",
      "idx: 6900\n",
      "idx: 7000\n",
      "idx: 7100\n",
      "idx: 7200\n",
      "idx: 7300\n",
      "idx: 7400\n",
      "idx: 7500\n",
      "idx: 7600\n",
      "idx: 7700\n",
      "idx: 7800\n",
      "idx: 7900\n",
      "idx: 8000\n",
      "idx: 8100\n",
      "idx: 8200\n",
      "idx: 8300\n",
      "idx: 8400\n",
      "idx: 8500\n",
      "idx: 8600\n",
      "idx: 8700\n",
      "idx: 8800\n",
      "idx: 8900\n",
      "idx: 9000\n",
      "idx: 9100\n",
      "idx: 9200\n",
      "idx: 9300\n",
      "idx: 9400\n",
      "idx: 9500\n",
      "idx: 9600\n",
      "idx: 9700\n",
      "idx: 9800\n",
      "idx: 9900\n",
      "idx: 10000\n",
      "idx: 10100\n",
      "idx: 10200\n",
      "idx: 10300\n",
      "idx: 10400\n",
      "idx: 10500\n",
      "idx: 10600\n",
      "idx: 10700\n",
      "idx: 10800\n",
      "idx: 10900\n",
      "idx: 11000\n",
      "idx: 11100\n",
      "idx: 11200\n",
      "idx: 11300\n",
      "idx: 11400\n",
      "idx: 11500\n",
      "idx: 11600\n",
      "idx: 11700\n",
      "idx: 11800\n",
      "idx: 11900\n",
      "idx: 12000\n",
      "idx: 12100\n",
      "idx: 12200\n",
      "idx: 12300\n",
      "idx: 12400\n",
      "idx: 12500\n",
      "idx: 12600\n",
      "idx: 12700\n",
      "idx: 12800\n",
      "idx: 12900\n",
      "idx: 13000\n",
      "idx: 13100\n",
      "idx: 13200\n",
      "idx: 13300\n",
      "idx: 13400\n",
      "idx: 13500\n",
      "idx: 13600\n",
      "idx: 13700\n",
      "idx: 13800\n",
      "idx: 13900\n",
      "idx: 14000\n",
      "idx: 14100\n",
      "idx: 14200\n",
      "idx: 14300\n",
      "idx: 14400\n",
      "idx: 14500\n",
      "idx: 14600\n",
      "idx: 14700\n",
      "idx: 14800\n",
      "idx: 14900\n",
      "idx: 15000\n",
      "idx: 15100\n",
      "idx: 15200\n",
      "idx: 15300\n",
      "idx: 15400\n",
      "idx: 15500\n",
      "idx: 15600\n",
      "idx: 15700\n",
      "idx: 15800\n",
      "idx: 15900\n",
      "idx: 16000\n",
      "idx: 16100\n",
      "idx: 16200\n",
      "idx: 16300\n",
      "idx: 16400\n",
      "idx: 16500\n",
      "idx: 16600\n",
      "idx: 16700\n",
      "idx: 16800\n",
      "idx: 16900\n",
      "idx: 17000\n",
      "idx: 17100\n",
      "idx: 17200\n",
      "idx: 17300\n",
      "idx: 17400\n",
      "idx: 17500\n",
      "idx: 17600\n",
      "idx: 17700\n",
      "idx: 17800\n",
      "idx: 17900\n",
      "idx: 18000\n",
      "idx: 18100\n",
      "idx: 18200\n",
      "idx: 18300\n",
      "idx: 18400\n",
      "idx: 18500\n",
      "idx: 18600\n",
      "idx: 18700\n",
      "idx: 18800\n",
      "idx: 18900\n",
      "idx: 19000\n",
      "idx: 19100\n",
      "idx: 19200\n",
      "idx: 19300\n",
      "idx: 19400\n",
      "idx: 19500\n",
      "idx: 19600\n",
      "idx: 19700\n",
      "idx: 19800\n",
      "idx: 19900\n",
      "idx: 20000\n",
      "idx: 20100\n",
      "idx: 20200\n",
      "idx: 20300\n",
      "idx: 20400\n",
      "idx: 20500\n",
      "idx: 20600\n",
      "idx: 20700\n",
      "idx: 20800\n",
      "idx: 20900\n",
      "idx: 21000\n",
      "idx: 21100\n",
      "idx: 21200\n",
      "idx: 21300\n",
      "idx: 21400\n",
      "idx: 21500\n",
      "idx: 21600\n",
      "idx: 21700\n",
      "idx: 21800\n",
      "idx: 21900\n",
      "idx: 22000\n",
      "idx: 22100\n",
      "idx: 22200\n",
      "idx: 22300\n",
      "idx: 22400\n",
      "idx: 22500\n",
      "idx: 22600\n",
      "idx: 22700\n",
      "idx: 22800\n",
      "idx: 22900\n",
      "idx: 23000\n",
      "idx: 23100\n",
      "idx: 23200\n",
      "idx: 23300\n",
      "idx: 23400\n",
      "idx: 23500\n",
      "idx: 23600\n",
      "idx: 23700\n",
      "idx: 23800\n",
      "idx: 23900\n",
      "idx: 24000\n",
      "idx: 24100\n",
      "idx: 24200\n",
      "idx: 24300\n",
      "idx: 24400\n",
      "idx: 24500\n",
      "idx: 24600\n",
      "idx: 24700\n",
      "idx: 24800\n",
      "idx: 24900\n",
      "idx: 25000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6f576fd3ef03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msrc_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'>>{target_lang}<< {comment_text}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print(src_txt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_translation_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_txt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtarget_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtarget_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m             )\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_generate_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, decoder_start_token_id, batch_size, num_return_sequences, length_penalty, num_beams, vocab_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             )\n\u001b[0;32m-> 1362\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size * num_beams, cur_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (batch_size * num_beams, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, decoder_cached_states, lm_labels, use_cache, **unused)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0mdecoder_cached_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_cached_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m         )\n\u001b[1;32m    956\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, encoder_outputs, decoder_attention_mask, decoder_cached_states, use_cache)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0mdecoder_causal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mdecoder_cached_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_cached_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m         \u001b[0;31m# Attention and hidden_states will be [] or None if they aren't needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, encoder_hidden_states, encoder_padding_mask, decoder_padding_mask, decoder_causal_mask, decoder_cached_states, use_cache, **unused)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mdecoder_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mlayer_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_causal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             )\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_hidden_states, encoder_attn_mask, layer_state, causal_mask, decoder_padding_mask)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/HU/lib/python3.7/site-packages/transformers/activations.py\u001b[0m in \u001b[0;36mswish\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mswish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_lang = 'it'\n",
    "targets = []\n",
    "print_every = 100\n",
    "for idx, row in df.iterrows():\n",
    "    target_row = row.copy()\n",
    "    comment_text = row[\"comment_text\"].replace(\"\\n\", \" \")\n",
    "    comment_text = \" \".join(comment_text.split(\" \")[0:250]).strip()\n",
    "    src_txt = f'>>{target_lang}<< {comment_text}'\n",
    "    translated = model.generate(**tokenizer.prepare_translation_batch([src_txt]))\n",
    "    target_txt = \" \".join([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n",
    "    target_row['comment_text'] = target_txt\n",
    "    target_row['lang'] = target_lang\n",
    "    targets.append(target_row)\n",
    "\n",
    "    if idx % print_every == 0:\n",
    "        print(\"idx:\", idx)\n",
    "        pd.DataFrame(targets).to_csv(\"train_data_it.csv\", index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
